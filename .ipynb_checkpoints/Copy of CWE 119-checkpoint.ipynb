{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niQQeq25U90F"
   },
   "source": [
    "# Software Vulnerability Detection using Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvFTH63KU90J"
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkALv1diU90J"
   },
   "source": [
    "Converting the HDF5 files for training/validation/testing datasets to python pickle for ease of future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izh9EE9CU90J"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnPe5vbAU90K"
   },
   "outputs": [],
   "source": [
    "# 3 datasets available\n",
    "\n",
    "data = h5py.File(\"VDISC_train.hdf5\",'r')\n",
    "#data = h5py.File(\"VDISC_validate.hdf5\",'r')\n",
    "#data = h5py.File(\"VDISC_test.hdf5\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir0lr4SkU90L",
    "outputId": "bf5e9d04-d45a-487d-8dc4-139876fd372c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE-119\n",
      "CWE-120\n",
      "CWE-469\n",
      "CWE-476\n",
      "CWE-other\n",
      "functionSource\n"
     ]
    }
   ],
   "source": [
    "# List all groups\n",
    "data.visit(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5Dmxh8mU90M"
   },
   "source": [
    "Create a new dataframe from the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BwtvEXSU90M"
   },
   "outputs": [],
   "source": [
    "mydf = pd.DataFrame(list(data['functionSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dXEv8L6U90N"
   },
   "outputs": [],
   "source": [
    "mydf['CWE-119']=list(data['CWE-119']); mydf['CWE-120']=list(data['CWE-120']); mydf['CWE-469']=list(data['CWE-469']); mydf['CWE-476']=list(data['CWE-476']); mydf['CWE-other']=list(data['CWE-other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jCVuExVU90N"
   },
   "outputs": [],
   "source": [
    "mydf.rename(columns={0:'functionSource'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XulapY6xU90N",
    "outputId": "536db98a-5079-4b11-da19-3f2e1bf5eb54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionSource</th>\n",
       "      <th>CWE-119</th>\n",
       "      <th>CWE-120</th>\n",
       "      <th>CWE-469</th>\n",
       "      <th>CWE-476</th>\n",
       "      <th>CWE-other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clear_area(int startx, int starty, int xsize, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ReconstructDuList(Statement* head)\\n{\\n    Sta...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free_speaker(void)\\n{\\n   if(Lengths)\\n       ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlx4_register_device(struct mlx4_dev *dev)\\n{\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parse_Env_Var(void)\\n{\\n  char *p = getenv(\"LI...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      functionSource  CWE-119  CWE-120  \\\n",
       "0  clear_area(int startx, int starty, int xsize, ...    False    False   \n",
       "1  ReconstructDuList(Statement* head)\\n{\\n    Sta...    False    False   \n",
       "2  free_speaker(void)\\n{\\n   if(Lengths)\\n       ...    False    False   \n",
       "3  mlx4_register_device(struct mlx4_dev *dev)\\n{\\...    False    False   \n",
       "4  Parse_Env_Var(void)\\n{\\n  char *p = getenv(\"LI...     True     True   \n",
       "\n",
       "   CWE-469  CWE-476  CWE-other  \n",
       "0    False    False      False  \n",
       "1    False    False      False  \n",
       "2    False    False      False  \n",
       "3    False    False      False  \n",
       "4    False    False       True  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.iloc[0:5,0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av1m8GSCU90N"
   },
   "outputs": [],
   "source": [
    "mydf.to_pickle(\"VDISC_train.pickle\")\n",
    "#mydf.to_pickle(\"VDISC_validate.pickle\")\n",
    "#mydf.to_pickle(\"VDISC_test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUB5yGWQU90N"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC7TpLNqU90N"
   },
   "source": [
    "### Importing processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHuYSY8FU90N"
   },
   "outputs": [],
   "source": [
    "train=pd.read_pickle(\"VDISC_train.pickle\")\n",
    "validate=pd.read_pickle(\"VDISC_validate.pickle\")\n",
    "test=pd.read_pickle(\"VDISC_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mkQWbvHU90O"
   },
   "outputs": [],
   "source": [
    "### CONTINUE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tU6c9xSU90O"
   },
   "source": [
    "## Learning Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2ofoRW6U90O"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpGkxh60U90O",
    "outputId": "e8d73273-88cb-4ee8-b092-bacc56243a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorlfow version:  2.0.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import pickle\n",
    "\n",
    "print(\"Tensorlfow version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkn4SMOKU90O"
   },
   "source": [
    "### Setting static and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWN6eCKlU90O",
    "outputId": "332e30f3-49c9-434f-bf35-33de830198fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is: 71926\n"
     ]
    }
   ],
   "source": [
    "# Generate random seed\n",
    "#myrand=np.random.randint(1, 99999 + 1)\n",
    "myrand=71926\n",
    "np.random.seed(myrand)\n",
    "tf.random.set_seed(myrand)\n",
    "print(\"Random seed is:\",myrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NpUHLqBU90P"
   },
   "outputs": [],
   "source": [
    "# Set the global value\n",
    "WORDS_SIZE=10000\n",
    "INPUT_SIZE=500\n",
    "NUM_CLASSES=2\n",
    "MODEL_NUM=0\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFmmUpd_U90P"
   },
   "source": [
    "### Importing processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36is3JMUU90P"
   },
   "outputs": [],
   "source": [
    "train=pd.read_pickle(\"VDISC_train.pickle\")\n",
    "validate=pd.read_pickle(\"VDISC_validate.pickle\")\n",
    "test=pd.read_pickle(\"VDISC_test.pickle\")\n",
    "\n",
    "for dataset in [train,validate,test]:\n",
    "    for col in range(1,6):\n",
    "        dataset.iloc[:,col] = dataset.iloc[:,col].map({False: 0, True: 1})\n",
    "\n",
    "# Create source code sdata for tokenization\n",
    "x_all = train['functionSource']\n",
    "#x_all = x_all.append(validate['functionSource'])\n",
    "#x_all = x_all.append(test['functionSource'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CHUdni8U90P",
    "outputId": "547305fc-6dfb-4d2d-aa6d-b0d3b364bdf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000185\n",
       "1      19286\n",
       "Name: CWE-119, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the datasets\n",
    "pd.value_counts(train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyyh58nRU90P"
   },
   "outputs": [],
   "source": [
    "one = train[train.iloc[:,1]==1].index.values.astype(int)\n",
    "zero = train[train.iloc[:,1]==0].index.values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZDmr3_MU90P"
   },
   "source": [
    "### Tokenizing the source codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPbdRfUeU90P",
    "outputId": "432dc8d7-a5d0-4c76-f424-5e1cfdb4acf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  1094129\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer with word-level\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(list(x_all))\n",
    "del(x_all)\n",
    "print('Number of tokens: ',len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J1EnhKqU90P"
   },
   "outputs": [],
   "source": [
    "# Reducing to top N words\n",
    "tokenizer.num_words = WORDS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEIj5zijU90Q",
    "outputId": "79b9cf9f-52ae-4ac6-8db6-3b58c7466fad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('if', 3908040),\n",
       " ('0', 2633095),\n",
       " ('return', 2182544),\n",
       " ('i', 1720280),\n",
       " ('1', 1483872),\n",
       " ('int', 1271988),\n",
       " ('null', 1222633),\n",
       " ('the', 990541),\n",
       " ('t', 917046),\n",
       " ('n', 892342)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words\n",
    "sorted(tokenizer.word_counts.items(), key=lambda x:x[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsA4W_OqU90Q"
   },
   "source": [
    "### Create sequence files from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIsF7PCgU90Q"
   },
   "outputs": [],
   "source": [
    "## Tokkenizing train data and create matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train['functionSource'])\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_train,\n",
    "                                  maxlen=INPUT_SIZE,\n",
    "                                  padding='post')\n",
    "x_train = x_train.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNbksfwlU90Q"
   },
   "outputs": [],
   "source": [
    "## Tokkenizing test data and create matrix\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test['functionSource'])\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_test,\n",
    "                                 maxlen=INPUT_SIZE,\n",
    "                                 padding='post')\n",
    "x_test = x_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt28daEvU90Q"
   },
   "outputs": [],
   "source": [
    "## Tokkenizing validate data and create matrix\n",
    "list_tokenized_validate = tokenizer.texts_to_sequences(validate['functionSource'])\n",
    "x_validate = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_validate,\n",
    "                                 maxlen=INPUT_SIZE,\n",
    "                                 padding='post')\n",
    "x_validate = x_validate.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBqigQN9U90Q",
    "outputId": "77c517b2-cd57-46ce-fd53-e7a96d901f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0    1000185\n",
      "1      19286\n",
      "Name: CWE-119, dtype: int64\n",
      "\n",
      "Test 0    124967\n",
      "1      2452\n",
      "Name: CWE-119, dtype: int64\n",
      "\n",
      "Validate 0    125057\n",
      "1      2419\n",
      "Name: CWE-119, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "print('Train', pd.value_counts(train.iloc[:,1]))\n",
    "print('\\nTest', pd.value_counts(test.iloc[:,1]))\n",
    "print('\\nValidate', pd.value_counts(validate.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ-rQlxHU90Q"
   },
   "source": [
    "### One-Hot-Enconding (OHE) on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGeby0DSU90Q"
   },
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "y_test=[]\n",
    "y_validate=[]\n",
    "\n",
    "for col in range(1,6):\n",
    "    y_train.append(tf.keras.utils.to_categorical(train.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))\n",
    "    y_test.append(tf.keras.utils.to_categorical(test.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))\n",
    "    y_validate.append(tf.keras.utils.to_categorical(validate.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac-QLWVOU90R",
    "outputId": "17f33c67-1ddc-4139-8c34-28c7ef875ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    124967\n",
       "1      2452\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example data\n",
    "pd.value_counts(y_test[0][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM0N49v7U90R"
   },
   "source": [
    "### Model Definition (CNN with Gaussian Noise and 1 Output Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fOe7gGeU90R"
   },
   "outputs": [],
   "source": [
    "# Create a random weights matrix\n",
    "\n",
    "random_weights = np.random.normal(size=(WORDS_SIZE, 13),scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKsjCPp9U90S",
    "outputId": "34434e77-89e7-4fe0-ca69-408316a42042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model built: \n",
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 13)           130000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 512)          60416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3276864   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,468,337\n",
      "Trainable params: 3,468,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Must use non-sequential model building to create branches in the output layer\n",
    "model = tf.keras.Sequential(name=\"CNN\")\n",
    "model.add(tf.keras.layers.Embedding(input_dim = WORDS_SIZE,\n",
    "                                    output_dim = 13,\n",
    "                                    weights=[random_weights],\n",
    "                                    input_length = INPUT_SIZE))\n",
    "#model.add(tf.keras.layers.GaussianNoise(stddev=0.01))\n",
    "model.add(tf.keras.layers.Convolution1D(filters=512, kernel_size=(9), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool1D(pool_size=5))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Define custom optimizers\n",
    "adam = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1, decay=0.0, amsgrad=False)\n",
    "\n",
    "## Compile model with metrics\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"CNN model built: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3mewjHDU90S"
   },
   "source": [
    "### Tensorboard Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-jIbhSjU90S"
   },
   "outputs": [],
   "source": [
    "## Create TensorBoard callbacks\n",
    "\n",
    "callbackdir= 'D:\\\\temp\\\\cb'\n",
    "\n",
    "tbCallback = tf.keras.callbacks.TensorBoard(log_dir=callbackdir,\n",
    "                         histogram_freq=1,\n",
    "                         embeddings_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True)\n",
    "\n",
    "tbCallback.set_model(model)\n",
    "mld = 'model/model-epoch-100-{epoch:02d}-single.hdf5'\n",
    "\n",
    "## Create best model callback\n",
    "mcp = tf.keras.callbacks.ModelCheckpoint(filepath=mld,\n",
    "                                         monitor=\"val_loss\",\n",
    "                                         save_best_only=True,\n",
    "                                         mode='auto',\n",
    "                                         save_freq='epoch',\n",
    "                                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRKn_a-CU90S"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZXYpES_U90S",
    "outputId": "f2cf1cb3-f1db-43e1-9a93-b165426788a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019471 samples, validate on 127476 samples\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.171570). Check your callbacks.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30277, saving model to model/model-epoch-100-01-single.hdf5\n",
      "1019471/1019471 - 173s - loss: 0.3170 - accuracy: 0.9805 - val_loss: 0.3028 - val_accuracy: 0.9810\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30277 to 0.25846, saving model to model/model-epoch-100-02-single.hdf5\n",
      "1019471/1019471 - 166s - loss: 0.2908 - accuracy: 0.9811 - val_loss: 0.2585 - val_accuracy: 0.9806\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25846 to 0.21230, saving model to model/model-epoch-100-03-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.2212 - accuracy: 0.9668 - val_loss: 0.2123 - val_accuracy: 0.9670\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21230 to 0.20217, saving model to model/model-epoch-100-04-single.hdf5\n",
      "1019471/1019471 - 169s - loss: 0.2041 - accuracy: 0.9644 - val_loss: 0.2022 - val_accuracy: 0.9612\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20217 to 0.19732, saving model to model/model-epoch-100-05-single.hdf5\n",
      "1019471/1019471 - 168s - loss: 0.1968 - accuracy: 0.9619 - val_loss: 0.1973 - val_accuracy: 0.9646\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19732 to 0.19551, saving model to model/model-epoch-100-06-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1921 - accuracy: 0.9615 - val_loss: 0.1955 - val_accuracy: 0.9568\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19551 to 0.19116, saving model to model/model-epoch-100-07-single.hdf5\n",
      "1019471/1019471 - 169s - loss: 0.1878 - accuracy: 0.9611 - val_loss: 0.1912 - val_accuracy: 0.9640\n",
      "Epoch 8/40\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19116 to 0.18985, saving model to model/model-epoch-100-08-single.hdf5\n",
      "1019471/1019471 - 167s - loss: 0.1838 - accuracy: 0.9614 - val_loss: 0.1899 - val_accuracy: 0.9557\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.18985 to 0.18429, saving model to model/model-epoch-100-09-single.hdf5\n",
      "1019471/1019471 - 166s - loss: 0.1804 - accuracy: 0.9617 - val_loss: 0.1843 - val_accuracy: 0.9607\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18429 to 0.18311, saving model to model/model-epoch-100-10-single.hdf5\n",
      "1019471/1019471 - 163s - loss: 0.1772 - accuracy: 0.9617 - val_loss: 0.1831 - val_accuracy: 0.9598\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18311 to 0.18015, saving model to model/model-epoch-100-11-single.hdf5\n",
      "1019471/1019471 - 168s - loss: 0.1738 - accuracy: 0.9623 - val_loss: 0.1802 - val_accuracy: 0.9638\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.18015 to 0.17869, saving model to model/model-epoch-100-12-single.hdf5\n",
      "1019471/1019471 - 167s - loss: 0.1699 - accuracy: 0.9628 - val_loss: 0.1787 - val_accuracy: 0.9686\n",
      "Epoch 13/40\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.17869 to 0.17245, saving model to model/model-epoch-100-13-single.hdf5\n",
      "1019471/1019471 - 169s - loss: 0.1658 - accuracy: 0.9636 - val_loss: 0.1725 - val_accuracy: 0.9652\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.17245 to 0.16869, saving model to model/model-epoch-100-14-single.hdf5\n",
      "1019471/1019471 - 169s - loss: 0.1618 - accuracy: 0.9648 - val_loss: 0.1687 - val_accuracy: 0.9630\n",
      "Epoch 15/40\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.16869\n",
      "1019471/1019471 - 169s - loss: 0.1580 - accuracy: 0.9656 - val_loss: 0.1696 - val_accuracy: 0.9580\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.16869 to 0.16333, saving model to model/model-epoch-100-16-single.hdf5\n",
      "1019471/1019471 - 171s - loss: 0.1545 - accuracy: 0.9663 - val_loss: 0.1633 - val_accuracy: 0.9663\n",
      "Epoch 17/40\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.16333 to 0.16196, saving model to model/model-epoch-100-17-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1518 - accuracy: 0.9669 - val_loss: 0.1620 - val_accuracy: 0.9688\n",
      "Epoch 18/40\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.16196 to 0.15875, saving model to model/model-epoch-100-18-single.hdf5\n",
      "1019471/1019471 - 172s - loss: 0.1492 - accuracy: 0.9672 - val_loss: 0.1587 - val_accuracy: 0.9659\n",
      "Epoch 19/40\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15875\n",
      "1019471/1019471 - 170s - loss: 0.1465 - accuracy: 0.9676 - val_loss: 0.1631 - val_accuracy: 0.9709\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.15875 to 0.15821, saving model to model/model-epoch-100-20-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1444 - accuracy: 0.9678 - val_loss: 0.1582 - val_accuracy: 0.9680\n",
      "Epoch 21/40\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15821\n",
      "1019471/1019471 - 169s - loss: 0.1429 - accuracy: 0.9677 - val_loss: 0.1588 - val_accuracy: 0.9718\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.15821 to 0.15506, saving model to model/model-epoch-100-22-single.hdf5\n",
      "1019471/1019471 - 172s - loss: 0.1407 - accuracy: 0.9679 - val_loss: 0.1551 - val_accuracy: 0.9679\n",
      "Epoch 23/40\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.15506 to 0.15476, saving model to model/model-epoch-100-23-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1392 - accuracy: 0.9681 - val_loss: 0.1548 - val_accuracy: 0.9683\n",
      "Epoch 24/40\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15476\n",
      "1019471/1019471 - 171s - loss: 0.1379 - accuracy: 0.9684 - val_loss: 0.1593 - val_accuracy: 0.9710\n",
      "Epoch 25/40\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15476\n",
      "1019471/1019471 - 170s - loss: 0.1363 - accuracy: 0.9684 - val_loss: 0.1567 - val_accuracy: 0.9689\n",
      "Epoch 26/40\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.15476 to 0.15259, saving model to model/model-epoch-100-26-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1348 - accuracy: 0.9686 - val_loss: 0.1526 - val_accuracy: 0.9679\n",
      "Epoch 27/40\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 168s - loss: 0.1336 - accuracy: 0.9688 - val_loss: 0.1533 - val_accuracy: 0.9679\n",
      "Epoch 28/40\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 172s - loss: 0.1323 - accuracy: 0.9689 - val_loss: 0.1562 - val_accuracy: 0.9706\n",
      "Epoch 29/40\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 172s - loss: 0.1309 - accuracy: 0.9691 - val_loss: 0.1553 - val_accuracy: 0.9708\n",
      "Epoch 30/40\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 169s - loss: 0.1300 - accuracy: 0.9692 - val_loss: 0.1553 - val_accuracy: 0.9700\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 170s - loss: 0.1287 - accuracy: 0.9695 - val_loss: 0.1526 - val_accuracy: 0.9655\n",
      "Epoch 32/40\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 172s - loss: 0.1274 - accuracy: 0.9697 - val_loss: 0.1571 - val_accuracy: 0.9704\n",
      "Epoch 33/40\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.15259\n",
      "1019471/1019471 - 172s - loss: 0.1261 - accuracy: 0.9698 - val_loss: 0.1528 - val_accuracy: 0.9659\n",
      "Epoch 34/40\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.15259 to 0.15180, saving model to model/model-epoch-100-34-single.hdf5\n",
      "1019471/1019471 - 172s - loss: 0.1251 - accuracy: 0.9699 - val_loss: 0.1518 - val_accuracy: 0.9672\n",
      "Epoch 35/40\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15180\n",
      "1019471/1019471 - 169s - loss: 0.1240 - accuracy: 0.9701 - val_loss: 0.1561 - val_accuracy: 0.9670\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.15180 to 0.15110, saving model to model/model-epoch-100-36-single.hdf5\n",
      "1019471/1019471 - 170s - loss: 0.1229 - accuracy: 0.9701 - val_loss: 0.1511 - val_accuracy: 0.9669\n",
      "Epoch 37/40\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15110\n",
      "1019471/1019471 - 170s - loss: 0.1218 - accuracy: 0.9705 - val_loss: 0.1577 - val_accuracy: 0.9707\n",
      "Epoch 38/40\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15110\n",
      "1019471/1019471 - 171s - loss: 0.1207 - accuracy: 0.9705 - val_loss: 0.1527 - val_accuracy: 0.9695\n",
      "Epoch 39/40\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15110\n",
      "1019471/1019471 - 171s - loss: 0.1200 - accuracy: 0.9704 - val_loss: 0.1559 - val_accuracy: 0.9700\n",
      "Epoch 40/40\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15110\n",
      "1019471/1019471 - 171s - loss: 0.1184 - accuracy: 0.9710 - val_loss: 0.1589 - val_accuracy: 0.9718\n"
     ]
    }
   ],
   "source": [
    "#ceiling = 38572\n",
    "#ceiling = ‭19286\n",
    "ceiling = 1019471\n",
    "\n",
    "class_weights = {0: 1., 1: 5.}\n",
    "\n",
    "history = model.fit(x = x_train[[*one,*zero[0:ceiling]],:],\n",
    "          y = train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(),\n",
    "          validation_data = (x_validate, validate.iloc[:,1].to_numpy()),\n",
    "          epochs = 40,\n",
    "          batch_size = 128,\n",
    "          verbose =2,\n",
    "          class_weight= class_weights,\n",
    "          callbacks=[mcp,tbCallback])\n",
    "\n",
    "with open('history/history-Epoch40-CNN-single', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Jq2fkvPiU90T"
   },
   "source": [
    "### Code to Continue Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEWVhaE3U90T"
   },
   "outputs": [],
   "source": [
    "# Continue training for another 100 epochs\n",
    "\n",
    "history40 = model.fit(x = x_train[[*one,*zero[0:ceiling]],:],\n",
    "          y = train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(),\n",
    "          validation_data = (x_validate, validate.iloc[:,1].to_numpy()),\n",
    "          epochs = 20,\n",
    "          batch_size = 128,\n",
    "          verbose =2,\n",
    "          class_weight= class_weights,\n",
    "          callbacks=[mcp,tbCallback])\n",
    "\n",
    "with open('history/history-Epoch40-CNN-single', 'wb') as file_pi:\n",
    "    pickle.dump(history40.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToKFbRLqU90T"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRwJVt6PU90T"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = tf.keras.models.load_model(\"model/model-epoch-100-36-single.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jYlvtPjU90T"
   },
   "source": [
    "### Model Evaluation using Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWkjY07EU90T"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_train[[*one,*zero[0:ceiling]],:], train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), verbose=0, batch_size=128, )\n",
    "for num in range(0,len(model.metrics_names)):\n",
    "    print(model.metrics_names[num]+': '+str(results[num]))\n",
    "\n",
    "predicted = model.predict_classes(x_train[[*one,*zero[0:ceiling]],:])\n",
    "predicted_prob = model.predict(x_train[[*one,*zero[0:ceiling]],:])\n",
    "\n",
    "print('\\nConfusion Matrix')\n",
    "#predicted = model.predict_classes(x_test)\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)\n",
    "print(confusion)\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "\n",
    "## Performance measure\n",
    "print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)))\n",
    "print('Precision: '+ str(sklearn.metrics.precision_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)))\n",
    "print('Recall: '+ str(sklearn.metrics.recall_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)))\n",
    "print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)))\n",
    "print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_score=predicted_prob)))\n",
    "print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_score=predicted_prob)))\n",
    "print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=train.iloc[[*one,*zero[0:ceiling]],1].to_numpy(), y_pred=predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UXwRxOAU90U"
   },
   "source": [
    "### Model Evaluation using Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htBZutrpU90U",
    "outputId": "4fc73d1d-e09a-44d4-e686-6346a8fb57b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.07783142023938047\n",
      "accuracy: 0.96657485\n",
      "[[121376   3591]\n",
      " [   668   1784]]\n",
      "\n",
      "TP: 1784\n",
      "FP: 3591\n",
      "TN: 121376\n",
      "FN: 668\n",
      "\n",
      "Accuracy: 0.9665748436261468\n",
      "Precision: 0.33190697674418607\n",
      "Recall: 0.7275693311582382\n",
      "F-measure: 0.45585792768621436\n",
      "Precision-Recall AUC: 0.37103943026150804\n",
      "AUC: 0.9437150902128537\n",
      "MCC: 0.47762199896225743\n"
     ]
    }
   ],
   "source": [
    "## Test data\n",
    "\n",
    "results = model.evaluate(x_test, test.iloc[:,1].to_numpy(), batch_size=128)\n",
    "for num in range(0,len(model.metrics_names)):\n",
    "    print(model.metrics_names[num]+': '+str(results[num]))\n",
    "\n",
    "predicted = model.predict_classes(x_test)\n",
    "predicted_prob = model.predict(x_test)\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)\n",
    "print(confusion)\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "\n",
    "## Performance measure\n",
    "print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)))\n",
    "print('Precision: '+ str(sklearn.metrics.precision_score(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)))\n",
    "print('Recall: '+ str(sklearn.metrics.recall_score(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)))\n",
    "print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)))\n",
    "print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=test.iloc[:,1].to_numpy(), y_score=predicted_prob)))\n",
    "print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=test.iloc[:,1].to_numpy(), y_score=predicted_prob)))\n",
    "print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=test.iloc[:,1].to_numpy(), y_pred=predicted)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
